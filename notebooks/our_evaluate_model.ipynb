{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZjyFDWDZ3Ld"
      },
      "source": [
        "## In this notebook we will see how to evaluate on the following benchmarks:\n",
        "\n",
        "- Pittsburgh (pitts30k-val, pitts30k-test and pitts250k-test) [1]\n",
        "- MapillarySLS [2]\n",
        "- Cross Season [3]\n",
        "- ESSEX [3]\n",
        "- Inria Holidays [3]\n",
        "- Nordland [3]\n",
        "- SPED [3]\n",
        "\n",
        "[1] NetVLAD: CNN architecture for weakly supervised place recognition (https://github.com/Relja/netvlad)\n",
        "\n",
        "[2] Mapillary Street-Level Sequences: A Dataset for Lifelong Place Recognition (https://github.com/FrederikWarburg/mapillary_sls)\n",
        "\n",
        "[3] VPR-Bench: An Open-Source Visual Place Recognition Evaluation Framework with Quantifiable Viewpoint and Appearance Change (https://github.com/MubarizZaffar/VPR-Bench)\n",
        "\n",
        "You'll need to download Pittsburgh dataset from [1] (you need to email Relja for the dataset), MapillarySLS validation from [2]. For the other datasets, visit [3] for detail on their amazing benchmark, they also host those datasets on this link (https://surfdrive.surf.nl/files/index.php/s/sbZRXzYe3l0v67W), huge thanks.\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** I rewrote the code for loading these datasets to ensure consistency in evaluation across all datasets and to improve its speed. The original code for these datasets was slow for valid reasons. For instance, VPR-Bench calculates multiple metrics, including latency, which requires individual image processing in the forward pass. MSLS offers various evaluation modes, such as Image_to_Image, Sequence_to_Sequence, Sequence_to_Image, among others. In this project, we focus solely on measuring recall@K and as a result, we can significantly speed up the validation process. Therefoe, you'll need to use the precomputed ground_truth that we provide in this repo (in the directory datasets).\n",
        "\n",
        "That being said, all you need to do is download the dataset and place it in a specific directory (we will need the dataset images). After that, you can hard-code the directory path into a global variable, as we will show in the following steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg8PEJteZ_k2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMSu6CIAZ9ry"
      },
      "outputs": [],
      "source": [
        "# !rm -rf '/content/drive/MyDrive/geoloc_fcm/geoloc-fcm-gsv-cities'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6Oqx4RDaQhf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !git clone https://github.com/fracrumatte/geoloc-fcm-gsv-cities.git  '/content/drive/MyDrive/geoloc_fcm/geoloc-fcm-gsv-cities'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DXkL2XRaTwu"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-metric-learning==1.6.3\n",
        "!pip install faiss-gpu==1.7.2\n",
        "!pip install pytorch-lightning==1.8.4\n",
        "!pip install timm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blwr-PhDZ3Lj"
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/geoloc_fcm/geoloc-fcm-gsv-cities') # append parent directory, we need it\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from utils.validation import get_validation_recalls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6lnC55EZ3Ln"
      },
      "outputs": [],
      "source": [
        "MEAN=[0.485, 0.456, 0.406]; STD=[0.229, 0.224, 0.225]\n",
        "\n",
        "IM_SIZE = (320, 320)\n",
        "\n",
        "def input_transform(image_size=IM_SIZE):\n",
        "    return T.Compose([\n",
        "        # T.Resize(image_size, interpolation=T.InterpolationMode.BICUBIC),\n",
        "\t\tT.Resize(image_size,  interpolation=T.InterpolationMode.BILINEAR),\n",
        "\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=MEAN, std=STD)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjCSJypoZ3Lp"
      },
      "source": [
        "In this project, we provide for each benchmark (or test dataset) a Dataset Class that encapsulates images sequentially as follows:\n",
        "\n",
        "$[R_1, R_2, ..., R_n, Q_1, Q_2, ..., Q_m]$ where $R_i$ are the reference images and $Q_i$ are the queries. We keep the number of references and queries as variables in the object so that we can split into references/queries later when evaluating. We also store a ground_truth matrix that indicates which references are prositives for each query.\n",
        "\n",
        "**Note:** make sure that for every [BenchmarkClass].py, the global variable DATASET_ROOT (where each dataset images are located) is well initialized, otherwise you won't be able to run the following steps. Also, GT_ROOT is the location of the precomputed ground_truth and filenames that WE PROVIDED (by default in ../datasets/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImuqM9POZ3Lp"
      },
      "outputs": [],
      "source": [
        "\n",
        "from dataloaders.val.SF_Dataset import SF_Dataset\n",
        "from dataloaders.val.TokyoDataset import Tokyo_Dataset\n",
        "\n",
        "\n",
        "\n",
        "def get_val_dataset(dataset_name, input_transform=input_transform()):\n",
        "    dataset_name = dataset_name.lower()\n",
        "\n",
        "    if 'cross' in dataset_name:\n",
        "        ds = CrossSeasonDataset(input_transform = input_transform)\n",
        "\n",
        "    elif 'essex' in dataset_name:\n",
        "        ds = EssexDataset(input_transform = input_transform)\n",
        "\n",
        "    elif 'inria' in dataset_name:\n",
        "        ds = InriaDataset(input_transform = input_transform)\n",
        "\n",
        "    elif 'nordland' in dataset_name:\n",
        "        ds = NordlandDataset(input_transform = input_transform)\n",
        "\n",
        "    elif 'sped' in dataset_name:\n",
        "        ds = SPEDDataset(input_transform = input_transform)\n",
        "\n",
        "    elif 'msls' in dataset_name:\n",
        "        ds = MSLS(input_transform = input_transform)\n",
        "\n",
        "    elif 'pitts' in dataset_name:\n",
        "        ds = PittsburghDataset(which_ds=dataset_name, input_transform = input_transform)\n",
        "    elif 'sf_val' in dataset_name:\n",
        "        ds = SF_Dataset(which_ds=dataset_name, input_transform = input_transform)\n",
        "    elif 'sf_test' in dataset_name:\n",
        "        ds = SF_Dataset(which_ds=dataset_name, input_transform = input_transform)\n",
        "    elif 'tokyo_test' in dataset_name:\n",
        "        ds = Tokyo_Dataset(which_ds=dataset_name, input_transform = input_transform)\n",
        "\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    num_references = ds.num_references\n",
        "    num_queries = ds.num_queries\n",
        "    ground_truth = ds.ground_truth\n",
        "    return ds, num_references, num_queries, ground_truth\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgf5-RcFZ3Lr"
      },
      "source": [
        "We define a function to which we give a model, a dataloader and it returns the resulting representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZCXOIZfZ3Ls"
      },
      "outputs": [],
      "source": [
        "def get_descriptors(model, dataloader, device):\n",
        "    descriptors = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, 'Calculating descritptors...'):\n",
        "            imgs, labels = batch\n",
        "            output = model(imgs.to(device)).cpu()\n",
        "            descriptors.append(output)\n",
        "\n",
        "    return torch.cat(descriptors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aS1vBNFZ3Ls"
      },
      "source": [
        "Let's now load a pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRGUS4DvZ3Lt"
      },
      "outputs": [],
      "source": [
        "\n",
        "from main import VPRModel\n",
        "# from main import trainer\n",
        "\n",
        "# define which device you'd like run experiments on (cuda:0 if you only have one gpu)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = VPRModel(backbone_arch='resnet18',\n",
        "                 layers_to_crop=[4],\n",
        "                #  agg_arch='ConvAP',\n",
        "                #  agg_config={'in_channels': 256,\n",
        "                #             'out_channels': 256,\n",
        "                #             's1' : 2,\n",
        "                #             's2' : 2},\n",
        "                  agg_arch='MixVPR',\n",
        "                  agg_config={'in_channels' : 256,\n",
        "                'in_h' : 20,\n",
        "                'in_w' : 20,\n",
        "                'out_channels' : 256,\n",
        "                'mix_depth' : 4,\n",
        "                'mlp_ratio' : 1,\n",
        "                'out_rows' : 4},\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "state_dict = torch.load('/content/drive/MyDrive/geoloc_fcm/LOGS/resnet18/lightning_logs/version_150/checkpoints/resnet18_epoch(09)_step(19520)_R1[0.7829]_R5[0.8595].ckpt')\n",
        "model.load_state_dict(state_dict['state_dict'])\n",
        "\n",
        "model.eval()\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjluPs_2Z3Lt"
      },
      "source": [
        "## Running validation on one of the benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X02f0h71Z3Lu"
      },
      "outputs": [],
      "source": [
        "# all_datasets = ['CrossSeason' ,'Essex' ,'Inria' ,'Nordland' ,'SPED' ,'MSLS']\n",
        "val_dataset_name = 'sf_val'  #mettere sf ??????????\n",
        "batch_size = 32\n",
        "\n",
        "val_dataset, num_references, num_queries, ground_truth = get_val_dataset(val_dataset_name)\n",
        "val_loader = DataLoader(val_dataset, num_workers=4, batch_size=batch_size)\n",
        "\n",
        "descriptors = get_descriptors(model, val_loader, device)\n",
        "print(f'Descriptor dimension {descriptors.shape[1]}')\n",
        "\n",
        "# now we split into references and queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FORMUeVvaeiF"
      },
      "outputs": [],
      "source": [
        "r_list = descriptors[ : num_references].cpu()\n",
        "q_list = descriptors[num_references : ].cpu()\n",
        "recalls_dict, preds = get_validation_recalls(r_list=r_list,\n",
        "                                    q_list=q_list,\n",
        "                                    k_values=[1, 5 , 10, 15, 20, 25], #[1, 5 , 10, 15, 20, 25]\n",
        "                                    gt=ground_truth,\n",
        "                                    print_results=True,\n",
        "                                    dataset_name=val_dataset_name,\n",
        "                                    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeM51YZwZ3Lv"
      },
      "source": [
        "## Evaluating on all benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44thLxDPZ3Lv"
      },
      "outputs": [],
      "source": [
        "# val_dataset_names = ['CrossSeason' ,'Essex' ,'Inria', 'MSLS', 'SPED', 'Nordland', 'pitts30k_test', 'pitts250k_test', 'sf', 'tokyo']\n",
        "val_dataset_names = ['tokyo_test','sf_test', 'sf_val']\n",
        "batch_size = 32\n",
        "\n",
        "for val_name in val_dataset_names:\n",
        "    val_dataset, num_references, num_queries, ground_truth = get_val_dataset(val_name)\n",
        "    val_loader = DataLoader(val_dataset, num_workers=4, batch_size=batch_size)\n",
        "    print(f'Evaluating on {val_name}')\n",
        "    descriptors = get_descriptors(model, val_loader, device)\n",
        "\n",
        "    print(f'Descriptor dimension {descriptors.shape[1]}')\n",
        "    r_list = descriptors[ : num_references]\n",
        "    q_list = descriptors[num_references : ]\n",
        "\n",
        "    recalls_dict, preds = get_validation_recalls(r_list=r_list,\n",
        "                                                q_list=q_list,\n",
        "                                                k_values=[1, 5, 10, 15, 20, 25],\n",
        "                                                gt=ground_truth,\n",
        "                                                print_results=True,\n",
        "                                                dataset_name=val_name,\n",
        "                                                faiss_gpu=False\n",
        "                                                )\n",
        "    del descriptors\n",
        "    print('========> DONE!\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEwGMmnjVPn_"
      },
      "outputs": [],
      "source": [
        "#Qualitative analysis\n",
        "gt=ground_truth\n",
        "k_values=[5]\n",
        "correct = {}\n",
        "for q_idx, pred in enumerate(preds):\n",
        "\n",
        "            for i, n in enumerate(k_values):\n",
        "                # if in top N then also in top NN, where NN > N\n",
        "                correct[q_idx]=[0,pred[:n]]\n",
        "                if np.any(np.in1d(pred[:n], gt[q_idx])):\n",
        "                    correct[q_idx] = [1,pred[:n]]\n",
        "                    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73Q_ogXgP7_e"
      },
      "outputs": [],
      "source": [
        "counter =0\n",
        "\n",
        "imgs_list=[]\n",
        "for i,val in enumerate(correct.values()):\n",
        "  # if val == 0:\n",
        "    counter+=1\n",
        "    imgs_list.append(val_dataset.qImages[i][7])\n",
        "\n",
        "imgs_list[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WPXb5tb3Xqg"
      },
      "outputs": [],
      "source": [
        "len(imgs_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PihlgD2Dm8d2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "fig, axes = plt.subplots(100,5, figsize=(10, 200))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    if(i<500):\n",
        "      img = plt.imread('/content/drive/MyDrive/geoloc_fcm/extracted_datasets/sf_xs/val/queries/'+imgs_list[i]+'.jpg')\n",
        "      ax.imshow(img)\n",
        "      ax.axis('off')\n",
        "      ax.set_title(f'Image {i}', fontsize=8)  # Aggiungi l'etichetta con l'indice dell'immagine\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# img = mpimg.imread('/content/drive/MyDrive/geoloc_fcm/extracted_datasets/sf_xs/val/queries/'+'@0553005.75@4174559.90@10@S@037.71676@-122.39858@F5LZ_39AImKuAaxYaZg_DQ@@180@@@@201704@@'+'.jpg')\n",
        "# plt.imshow(img)\n",
        "# plt.axis('off')\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge6GwO-7pBNc"
      },
      "outputs": [],
      "source": [
        "#comparing the two models, what our model rekognized and the other donesn't\n",
        "\n",
        "dict_our={}\n",
        "dict_mix={}\n",
        "\n",
        "lis_res=[]\n",
        "for i in range(len(preds)):\n",
        "  if( i in dict_our.keys and i in dict_mix.keys):\n",
        "    if(dict_our[i][0]==1 and dict_mix[i][0]==0):\n",
        "      lis_res.append(i)\n",
        "      lis_res.append(dict_our[i][1])\n",
        "      lis_res.append(dict_mix[i][1])\n",
        "\n",
        "l = len(lis_res) // 3\n",
        "\n",
        "fig, axes = plt.subplots(l,3, figsize=(6, 40))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    if(i<100):\n",
        "      img = plt.imread('/content/drive/MyDrive/geoloc_fcm/extracted_datasets/sf_xs/val/queries/'+lis_res[i]+'.jpg')\n",
        "      ax.imshow(img)\n",
        "      ax.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "13e28b63dcb11f8cc8ca6da5fb3a89358e6ee1c494e903d975090f8ad11f1453"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}